{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08fd03c",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">Space Debris Detection using detection transformer with custom backbone</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56630b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from transformers import DetrForObjectDetection, DetrImageProcessor, DetrConfig\n",
    "from transformers.models.detr.modeling_detr import DetrSinePositionEmbedding\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "\n",
    "from squeezenet_backbone import SqueezeNetBackbone\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ae93e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_float32_matmul_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e41a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this new class to your notebook cell\n",
    "\n",
    "\n",
    "class CustomBackboneJoiner(nn.Module):\n",
    "    def __init__(self, backbone: nn.Module, d_model: int):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.position_embedding = DetrSinePositionEmbedding(embedding_dim=d_model // 2, normalize=True)\n",
    "\n",
    "    def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):\n",
    "        # Get the feature map list from our SqueezeNet backbone\n",
    "        # SqueezeNet returns features_list, None\n",
    "        features_list, _ = self.backbone(pixel_values, pixel_mask)\n",
    "        \n",
    "        # Now, calculate positional embeddings for the feature map\n",
    "        pos_embeddings_list = []\n",
    "        for feat_tensor, mask in features_list:\n",
    "            # The position embedding module expects a specific dict format\n",
    "\n",
    "            pos_embeddings_list.append(self.position_embedding(feat_tensor, mask))\n",
    "            \n",
    "        return features_list, pos_embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706cb140",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"debris_det_dataset\"\n",
    "\n",
    "ANNOTATION_FILE_NAME = \"_annotations.coco.json\"\n",
    "TRAIN_DIR = os.path.join(dataset_path, \"train\")\n",
    "VAL_DIR = os.path.join(dataset_path, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7cc7861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of training samples: 20000\n",
      "Number of validation samples: 2000\n"
     ]
    }
   ],
   "source": [
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, image_dir_path:str, image_processor, train:bool=True):\n",
    "        annot_file_path = os.path.join(image_dir_path, ANNOTATION_FILE_NAME)\n",
    "        super(CocoDetection, self).__init__(image_dir_path, annot_file_path)\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images, annotations = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        annotations = {'image_id': image_id, 'annotations': annotations}\n",
    "        encoding = self.image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "        pixel_values = encoding['pixel_values'].squeeze()\n",
    "        target = encoding['labels'][0]\n",
    "        return pixel_values, target\n",
    "    \n",
    "\n",
    "TRAIN_DATASET = CocoDetection(TRAIN_DIR, DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\"), train=True)\n",
    "VAL_DATASET = CocoDetection(VAL_DIR, DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\"), train=False)\n",
    "\n",
    "print(f\"Number of training samples: {len(TRAIN_DATASET)}\")\n",
    "print(f\"Number of validation samples: {len(VAL_DATASET)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96443085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    encoding = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\").pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[1] for item in batch]\n",
    "    return {'pixel_values': encoding['pixel_values'], 'pixel_mask': encoding['pixel_mask'], 'labels': labels}\n",
    "\n",
    "\n",
    "TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET, collate_fn=collate_fn, batch_size=4, shuffle=True, num_workers=4)\n",
    "VAL_DATALOADER = DataLoader(dataset=VAL_DATASET, collate_fn=collate_fn, batch_size=4, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7852437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detr(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, lr, lr_backbone, weight_decay, backbone_name:str =None, set_pretrained_backbone:bool=True, use_timm_backbone:bool=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_timm_backbone = use_timm_backbone\n",
    "\n",
    "        if backbone_name==\"squeezenet\":\n",
    "            backbone = SqueezeNetBackbone(pretrained=set_pretrained_backbone)\n",
    "            print(\"Using SqueezeNet backbone\")\n",
    "\n",
    "            self.model = DetrForObjectDetection(DetrConfig(num_labels=1, num_queries=10))\n",
    "\n",
    "\n",
    "            d_model = self.model.config.d_model\n",
    "\n",
    "            backbone_joiner = CustomBackboneJoiner(backbone, d_model)\n",
    "\n",
    "            self.model.model.backbone = backbone_joiner\n",
    "            in_channels = backbone.num_channels\n",
    "            \n",
    "            self.model.model.input_projection = torch.nn.Conv2d(in_channels=in_channels, out_channels=d_model, kernel_size=1)\n",
    "            \n",
    "\n",
    "        elif backbone_name in timm.list_models():\n",
    "            print(f\"Using {backbone_name} backbone from timm\")\n",
    "\n",
    "            config = DetrConfig(\n",
    "                backbone=backbone_name,\n",
    "                use_timm_backbone=use_timm_backbone,\n",
    "                use_pretrained_backbone=set_pretrained_backbone,\n",
    "                num_queries=10,\n",
    "                num_labels=1, # COCO class count + 1 for \"no object\"\n",
    "                lr_backbone=lr_backbone,\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay,\n",
    "            )\n",
    "\n",
    "            self.model = DetrForObjectDetection(config)\n",
    "        else:\n",
    "            print(\"Using default DETR ResNet-50 backbone\")\n",
    "\n",
    "            self.model = DetrForObjectDetection.from_pretrained(\n",
    "                \"facebook/detr-resnet-50\",\n",
    "                num_labels=2, # COCO class count + 1 for \"no object\"\n",
    "                lr_backbone=lr_backbone,\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay,\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, pixel_values, pixel_mask): \n",
    "        return self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step, and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation/loss\", loss)\n",
    "        for k, v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # DETR authors decided to use different learning rate for backbone\n",
    "        # you can learn more about it here: \n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L22-L23\n",
    "        # - https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/main.py#L131-L139\n",
    "        param_dicts = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        return torch.optim.AdamW(param_dicts, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return TRAIN_DATALOADER\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return VAL_DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94184c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SqueezeNet backbone\n"
     ]
    }
   ],
   "source": [
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4, backbone_name=\"squeezenet\", set_pretrained_backbone=True, use_timm_backbone=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(TRAIN_DATALOADER))\n",
    "\n",
    "# outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc844aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3930ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                   | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model | DetrForObjectDetection | 18.4 M | train\n",
      "---------------------------------------------------------\n",
      "18.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.4 M    Total params\n",
      "73.412    Total estimated model params size (MB)\n",
      "248       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ded6268b43848959098ee0594f524a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER X\\Desktop\\debrisClassifier\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\USER X\\Desktop\\debrisClassifier\\venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 4. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\USER X\\Desktop\\debrisClassifier\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40e19711ecc4d8eae2a5fa2aa1b1d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER X\\Desktop\\debrisClassifier\\venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 50\n",
    "\n",
    "trainer = Trainer(\n",
    "    devices=1, \n",
    "    accelerator=\"gpu\", \n",
    "    max_epochs=max_epochs,\n",
    "    gradient_clip_val=0.1, \n",
    "    accumulate_grad_batches=8, \n",
    "    log_every_n_steps=5,\n",
    "    val_check_interval=0.25,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
